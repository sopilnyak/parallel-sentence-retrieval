{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "ttw_mining.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWYCP-WWeX_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eENSvCxDeX_e",
        "colab_type": "text"
      },
      "source": [
        "## Parallel data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV9PGTz0eX_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lang_source = 'en'\n",
        "lang_target = 'ru'\n",
        "raw_root = 'parallel'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6m2DGEozeX_h",
        "colab_type": "text"
      },
      "source": [
        "### TED + Tatoeba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM9mEplieX_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sents(raw_xml, id_prefix):\n",
        "    tree = ET.parse(raw_xml)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    sents = {}\n",
        "    for sent in root.iter('s'):\n",
        "        sents['{}_{}'.format(id_prefix, sent.attrib['id'])] = sent.text\n",
        "    return sents\n",
        "\n",
        "def clear_sents(sents):\n",
        "    new_sents = {}\n",
        "    for key, value in sents.items():\n",
        "        if value is not None and \\\n",
        "            'http' not in value and \\\n",
        "            len(value) > 5 and \\\n",
        "            not value.isnumeric() and \\\n",
        "            value.count(' ') > 0:\n",
        "            new_sents[key] = value\n",
        "    return new_sents\n",
        "\n",
        "def parse_parallel_corpus(corpus_name):\n",
        "\n",
        "    corpus_root = os.path.join(raw_root, corpus_name.lower())\n",
        "    raw_source = os.path.join(corpus_root, '{lang}.xml'.format(lang=lang_source))\n",
        "    raw_target = os.path.join(corpus_root, '{lang}.xml'.format(lang=lang_target))\n",
        "    index = os.path.join(corpus_root, '{source}-{target}.xml'.format(source=lang_source, target=lang_target))\n",
        "\n",
        "    def get_pairs(source_sents, target_sents, pair_xml, id_prefix=corpus_name.lower()):\n",
        "        tree = ET.parse(pair_xml)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        pairs = []\n",
        "        for link in root.iter('link'):\n",
        "            source_id, target_id = link.attrib['xtargets'].split(';')\n",
        "            source_id = '{}_{}'.format(id_prefix, source_id)\n",
        "            target_id = '{}_{}'.format(id_prefix, target_id)\n",
        "            if source_id in source_sents and target_id in target_sents:\n",
        "                pairs.append((source_id, target_id))\n",
        "        return pairs\n",
        "\n",
        "\n",
        "    source_sents = clear_sents(get_sents(raw_source, corpus_name.lower()))\n",
        "    target_sents = clear_sents(get_sents(raw_target, corpus_name.lower()))\n",
        "    pairs = get_pairs(source_sents, target_sents, index)\n",
        "    \n",
        "    return source_sents, target_sents, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8ClYA00eX_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ted_source, ted_target, ted_pairs = parse_parallel_corpus('TED2013')\n",
        "tatoeba_source, tatoeba_target, tatoeba_pairs = parse_parallel_corpus('Tatoeba')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt7zATlYeX_q",
        "colab_type": "text"
      },
      "source": [
        "### WMT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbYtnEMIeX_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_parallel_multiple_editions(corpus_name, edition_name):\n",
        "    corpus_root = os.path.join(raw_root, corpus_name.lower())\n",
        "    raw_source = os.path.join(corpus_root, '{edition_name}-{lang}.xml'\n",
        "                              .format(edition_name=edition_name, lang=lang_source))\n",
        "    raw_target = os.path.join(corpus_root, '{edition_name}-{lang}.xml'\n",
        "                              .format(edition_name=edition_name, lang=lang_target))\n",
        "    index = os.path.join(corpus_root, '{source}-{target}.xml'\n",
        "                         .format(source=lang_source, target=lang_target))\n",
        "\n",
        "    def get_pairs(source_sents, target_sents, pair_xml, id_prefix=edition_name.lower()):\n",
        "        tree = ET.parse(pair_xml)\n",
        "        root = tree.getroot()\n",
        "\n",
        "        pairs = []\n",
        "        for link_group in root.iter('linkGrp'):\n",
        "            if edition_name not in link_group.attrib['fromDoc']:\n",
        "                continue\n",
        "            for link in link_group.iter('link'):\n",
        "                source_id, target_id = link.attrib['xtargets'].split(';')\n",
        "                source_id = '{}_{}'.format(id_prefix, source_id)\n",
        "                target_id = '{}_{}'.format(id_prefix, target_id)\n",
        "                if source_id in source_sents and target_id in target_sents:\n",
        "                    pairs.append((source_id, target_id))\n",
        "        return pairs\n",
        "\n",
        "\n",
        "    source_sents = clear_sents(get_sents(raw_source, edition_name.lower()))\n",
        "    target_sents = clear_sents(get_sents(raw_target, edition_name.lower()))\n",
        "    pairs = get_pairs(source_sents, target_sents, index)\n",
        "    \n",
        "    return source_sents, target_sents, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN-nVK3HeX_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wmt_source = {}\n",
        "wmt_target = {}\n",
        "wmt_pairs = []\n",
        "for edition_year in [2015, 2016, 2017, 2018, 2019]:\n",
        "    source, target, pairs = parse_parallel_multiple_editions('wmt', 'newstest{}'.format(edition_year))\n",
        "    wmt_source.update(source)\n",
        "    wmt_target.update(target)\n",
        "    wmt_pairs += pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdyctvbyeX_w",
        "colab_type": "text"
      },
      "source": [
        "### Parallel data altogether"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA5D4-ySeX_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parallel_source = {}\n",
        "parallel_source.update(ted_source)\n",
        "parallel_source.update(tatoeba_source)\n",
        "parallel_source.update(wmt_source)\n",
        "\n",
        "parallel_target = {}\n",
        "parallel_target.update(ted_target)\n",
        "parallel_target.update(tatoeba_target)\n",
        "parallel_target.update(wmt_target)\n",
        "\n",
        "gold_pairs = ted_pairs + tatoeba_pairs + wmt_pairs\n",
        "\n",
        "print(len(gold_pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzayzo0FeX_2",
        "colab_type": "text"
      },
      "source": [
        "## Adding monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4fruA79eX_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monolingual_data = 'monolingual'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SChzL81EeX_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parse_monolingual(corpus_name, lang):\n",
        "    corpus_root = os.path.join(monolingual_data, corpus_name.lower())\n",
        "    raw = os.path.join(corpus_root, '{lang}.xml'.format(lang=lang))\n",
        "    source_sents = clear_sents(get_sents(raw, corpus_name.lower()))\n",
        "    return source_sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUltMfqDeX_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tedhren_monolingual_source = parse_monolingual('TedHrEn', lang_source)\n",
        "len(tedhren_monolingual_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW-tQePgeX__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_duplicates(mono, parallel):\n",
        "    mono_set = set(mono.values())\n",
        "    parallel_set = set(parallel.values())\n",
        "    intersection = mono_set.intersection(parallel_set)\n",
        "    cleaned_mono = {}\n",
        "    for key, value in mono.items():\n",
        "        if value not in intersection:\n",
        "            cleaned_mono[key] = value\n",
        "    return cleaned_mono"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pr8QAghMeYAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tedhren_monolingual_source = remove_duplicates(tedhren_monolingual_source, parallel_source)\n",
        "len(tedhren_monolingual_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlcQKU9keYAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reduce gold to get more monolingual data\n",
        "\n",
        "import random\n",
        "\n",
        "random.shuffle(gold_pairs)\n",
        "result_size = len(gold_pairs) // 2\n",
        "\n",
        "error_count = 0\n",
        "for i, (source_id, target_id) in enumerate(gold_pairs[result_size:]):\n",
        "    try:\n",
        "        if i % 2 == 0:\n",
        "            # Keep target sentence, remove source\n",
        "            del(parallel_source[source_id])\n",
        "        else:\n",
        "            del(parallel_target[target_id])\n",
        "    except KeyError:\n",
        "        error_count += 1\n",
        "        \n",
        "gold_pairs = gold_pairs[:result_size]\n",
        "print(len(gold_pairs))\n",
        "print(len(parallel_source))\n",
        "print(len(parallel_target))\n",
        "print(error_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8m0wyNc1eYAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fix gold by removing missing ids and identical pairs\n",
        "\n",
        "cleaned_gold_pairs = []\n",
        "for source_id, target_id in gold_pairs:\n",
        "    if source_id in parallel_source and target_id in parallel_target:\n",
        "        if parallel_source[source_id] != parallel_target[target_id]:\n",
        "            cleaned_gold_pairs.append((source_id, target_id))\n",
        "        else:\n",
        "            del parallel_source[source_id]\n",
        "            del parallel_target[target_id]\n",
        "gold_pairs = cleaned_gold_pairs\n",
        "len(cleaned_gold_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2qw-TqPeYAL",
        "colab_type": "code",
        "colab": {},
        "outputId": "07ef7008-c9f1-429f-94ff-5d65b01c82e6"
      },
      "source": [
        "# Sanity check\n",
        "\n",
        "for source_id, target_id in gold_pairs[:10]:\n",
        "    print('{}: {}'.format(source_id, parallel_source[source_id]))\n",
        "    print('{}: {}'.format(target_id, parallel_target[target_id]))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ted2013_45302: In fact, it's not even really sequenced that much.\n",
            "ted2013_45302: Его генетические последовательности выделены не полностью.\n",
            "\n",
            "ted2013_77290: You have time to -- you have time to alert one house. What do you do?\n",
            "ted2013_77290: И у Вас есть время только -- ... у Вас есть время оповестить только один из домов. Что Вы сделаете?\n",
            "\n",
            "ted2013_57527: And I think the imagination is the only limit of what you can think of when this kind of technology merges with real life.\n",
            "ted2013_57527: Думаю, что единственным ограничителем станет ваша фантазия когда эта технология проникнет в нашу жизнь.\n",
            "\n",
            "tatoeba_4495275: Why are you sad?\n",
            "tatoeba_4743980: Ты чего грустный?\n",
            "\n",
            "tatoeba_284753: He didn't dare say anything.\n",
            "tatoeba_3676761: Он не осмелился ничего сказать.\n",
            "\n",
            "tatoeba_2094165: I don't think she is right.\n",
            "tatoeba_5607112: Не думаю, что она права.\n",
            "\n",
            "tatoeba_2255418: You've convinced me.\n",
            "tatoeba_2694787: Ты меня убедил.\n",
            "\n",
            "tatoeba_7185599: When was the first time that Tom kissed you?\n",
            "tatoeba_6934366: Когда Том тебя впервые поцеловал?\n",
            "\n",
            "ted2013_83669: A small mutation can take a two-wing fly and make it a four-wing fly.\n",
            "ted2013_83669: Малая мутация способна превратить двукрылую муху в 4-х-крылую.\n",
            "\n",
            "tatoeba_2958349: Tom took off his muddy shoes.\n",
            "tatoeba_4458762: Том снял свою грязную обувь.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J00oUs06eYAO",
        "colab_type": "text"
      },
      "source": [
        "### Pickling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoIicVaheYAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "def save_data(data, name, pkl_root='ttw_corpus'):\n",
        "    with open(os.path.join(pkl_root, '{}.pkl').format(name), 'wb') as f:\n",
        "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1isQqGadeYAQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "02cfc415-ca4f-459a-f536-127bc7a60257"
      },
      "source": [
        "!mkdir ttw_corpus\n",
        "\n",
        "parallel_source.update(tedhren_monolingual_source)\n",
        "source = parallel_source\n",
        "target = parallel_target\n",
        "\n",
        "save_data(source, 'ttw_source')\n",
        "save_data(target, 'ttw_target')\n",
        "save_data(gold_pairs, 'ttw_gold')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "A subdirectory or file ttw_corpus already exists.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}